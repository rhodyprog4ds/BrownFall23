---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Regression


## Feedback

See prismia notes for feedback qualitative comments

## Setting up for regression


We're going to predict **tip** from **total bill**.
This is a regression problem because the target, *tip* is:
 1. available in the data (makes it supervised) and 
 2.  a continuous value.
The problems we've seen so far were all classification, species of iris and the
character in that corners data were both categorical.  

Using linear regression is also a good choice because it makes sense that the tip
would be approximately linearly related to the total bill, most people pick some
percentage of the total bill.  If we our prior knowledge was that people
typically tipped with some more complicated function, this would not be a good
model.

```{code-cell} ipython3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
sns.set_theme(font_scale=2,palette='colorblind').pre
```


We wil load this data from `seaborn`
```{code-cell} ipython3
tips_df = sns.load_dataset('tips')
tips_df.head()
```

Since we will use only one variable, we have to do some extra work.  When we use sklearn with a DataFrame, it picks out the values, like this: 
```{code-cell} ipython3
tips_df['total_bill'].values
```

This is because originally sklearn actualy only worked on `numpy` arrays and sot what they do now is pull it out of the DataFrame, it gives us a numpy array:

```{code-cell} ipython3
type(tips_df['total_bill'].values)
```

All sklearn estimator objects are designed to work on multiple features, which mean they expect that the features have a 2D shape, but when we pick out a single column's (pandas Series) values, we get a 1D shape: 
```{code-cell} ipython3
tips_df['total_bill'].values.shape
```

We can change this by adding an axis.  It doesn't change values, but changes the way it is represented enough that it has the properties we need. 


```{code-cell} ipython3
tips_X = tips_df['total_bill'].values[:,np.newaxis]
```

This has the shape we want:

```{code-cell} ipython3
tips_X.shape
```

and is still the same type
```{code-cell} ipython3
type(tips_X)
```


The target is expected to be a single feature, so we do not need to do anythign special. 
```{code-cell} ipython3
tips_y = tips_df['tip']
```


Next, we split the data
```{code-cell} ipython3
tips_X_train, tips_X_test, tips_y_train, tips_y_test = train_test_split(tips_X,tips_y)
```

## Fitting a regression model

We instantiate the object as we do with all other sklearn estimator objects. 

```{code-cell} ipython3
regr = linear_model.LinearRegression()
```

and `fit` liek the others too. 

```{code-cell} ipython3
regr.fit(tips_X_train, tips_y_train)
```

we also can predict as before

```{code-cell} ipython3
y_pred = regr.predict(tips_X_test)
```

## Regression Performance


```{code-cell} ipython3
mean_squared_error(tips_y_test,y_pred)
```

```{code-cell} ipython3
np.sqrt(mean_squared_error(tips_y_test,y_pred))
```

```{code-cell} ipython3
tips_y_test.describe()
```

```{code-cell} ipython3
regr.coef_
```

```{code-cell} ipython3
regr.intercept_
```

```{code-cell} ipython3
tips_X_test[0]*regr.coef_ + regr.intercept_
```

```{code-cell} ipython3
y_pred[0]
```

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.plot(tips_X_test,y_pred, color='blue')
```

```{code-cell} ipython3
plt.plot(tips_X_test, y_pred, color='blue', linewidth=3)

# draw vertical lines frome each data point to its predict value
[plt.plot([x,x],[yp,yt], color='red', linewidth=3)
                 for x, yp, yt in zip(tips_X_test, y_pred,tips_y_test)];
plt.scatter(tips_X_test, tips_y_test,  color='black')
```

```{code-cell} ipython3
r2_score(tips_y_test, y_pred)
```

```{code-cell} ipython3
x = 10*np.random.random(20)
y_pred = 3*x
ex_df = pd.DataFrame(data = x,columns = ['x'])
ex_df['y_pred'] = y_pred
n_levels = range(1,18,2)
noise = (np.random.random(20)-.5)*2
for n in n_levels:
    y_true = y_pred + n* noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true

f_x_list = [2*x,3.5*x,.5*x**2, .03*x**3, 10*np.sin(x)+x*3,3*np.log(x**2)]
for fx in f_x_list:
    y_true = fx + noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true    

xy_df = ex_df.melt(id_vars=['x','y_pred'],var_name='rscore',value_name='y')
# sns.lmplot(x='x',y='y', data = xy_df,col='rscore',col_wrap=3,)
g = sns.FacetGrid(data = xy_df,col='rscore',col_wrap=3,aspect=1.5,height=3)
g.map(plt.plot, 'x','y_pred',color='k')
g.map(sns.scatterplot, "x", "y",)
```

```{code-cell} ipython3
y_pred = regr.predict(tips_X_test)
```

```{code-cell} ipython3
mean_squared_error(tips_y_test,y_pred)
```

```{code-cell} ipython3
r2_score(tips_y_test,y_pred)
```

```{code-cell} ipython3
regr.score(tips_X_test,tips_y_test)
```

```{code-cell} ipython3

```

## Questions after class

### How do you know if a regression model is good or works for a dataset? 

Let's break this down.  Regression is supervised learning, so there needs to be variables that you can use as features and one to use as the target. To test if supervised learning makes sense, try filling column names in place of `features` and `target` in the following sentence, that describes a prediction task: we want to predict `target` from `features`.  

Regression, specifically means that the target variable needs to be continuous, not discrete. For example, in today's class, we predicted the tip in dollars, which is continous in this sense.  This is not stictly continuous in the mathematical sense, but anything that is most useful to consider as continuous. 


_logisitic regression is actually a classification model despite the name_

### So is regression is just used with numeric values in terms of predicting?

Yes linear regression requires numeric values for the features as well as the target.  

Other types of regression can use other types of features. 

All regression is for a continusous target. 

### What are the strengths of a regression model as opposed to the previous models we've seen?

Using regression, classification, or clustering is based on what type of problem you are solving. 

Our simple flowchart for what task you are doing is:

```{mermaid}
flowchart TD
    labels{Do you have labels <br>in the dataset? }
    labels --> |yes| sup[Supervised Learning]
    labels --> |no| unsup[Unsupervised Learning]
    sup --> ltype{What type of variable <br> are the labels?}
    ltype --> |continuous| reg[Regression]
    ltype --> |discrete| clas[Classification]
    unsup --> groups{Do you think there are <br> groups within the data?}
    groups --> |yes | clus[Clutering]
```

the [sklearn docs include a more detailed guide](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

### Are linear models exclusively used for regression problems?

No, there are also linear classification models. 

### What does a negative R2 score mean?

It's a bad fit. 

### What is the difference between a good and bad rscore?

1 is perfect, low is bad. What is "good enough" is context dependent. 

### why were your values coming out negative?

Mine was not a very good fit.  We'll see why on Thursday.  Yours may have been better

### What would be a common example of a real-world linear regression?

Lots of real models are linear regression, but with many inputs.  It is often not a perfect fit, but good enough. 

### what are the coefficient and intercept ?

The coefficient is the slope of the line and the intercept is the value of the label the model predicts for features =0. 

### how do you add string data thats not binary e.g 'True' 'False' to make it fit in data?

You have to transform it to be numerical.  If it is True and False, you can cast it to integers. 

I recommend for A9 filtering on the UCI website to pick out data with numerical features. 

In general, and you can for A9 if you want, use the [sklearn `LabelEncodeer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to transform the data.  