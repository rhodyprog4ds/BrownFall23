---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Clustering Metrics


## Comparing Classification and Clustering Data

+++


Datasets for classification must have a target variable observed, but we can drop it to use it for clustering

## KMeans review

- clustering goal: find groups of samples that are similar
- k-means assumption: a fixed number ($k$) of means will describe the data enough to find the groups


## Clustering with Sci-kit Learn


```{code-cell} ipython3
import seaborn as sns
import numpy as np
import matplotlib.pylab as plt
from sklearn import datasets
from sklearn.cluster import KMeans
from sklearn import metrics
import pandas as pd
sns.set_theme(palette='colorblind')

# set global random seed so that the notes are the same each time the site builds
np.random.seed(1103)
```


Today we will load the iris data from seaborn: 

```{code-cell} ipython3
iris_df = sns.load_dataset('iris')
```

```{code-cell} ipython3
iris_df.head()
```


Remember this is how the clustering algorithm sees the data, with no labels:

```{code-cell} ipython3
sns.pairplot(iris_df)
```


Next we need to create a copy of the data that's appropriate for clustering. Remember that clustering is *unsupervised* so it doesn't have a target variable. We also can do clustering on the data with or without splitting into test/train splits, since it doesn't use a target variable, we can evaluate how good the clusters it finds are on the actual data that it learned from.

+++


We can either pick the measurements out or drop the species column.
remember most data frame operations return a copy of the dataframe.


+++
```{code-cell} ipython3
iris_X = iris_df.drop(columns=['species'])
```

```{code-cell} ipython3
iris_X.head()
```

Next, we create a Kmeans estimator object with 3 clusters, since we know that the iris data has 3 species of flowers. We refer to these three groups as classes in classification (the goal is to label the classes...) and in clustering we sometimes borrow that word.  Sometimes, clustering literature will be more abstract and refer to partitions, this is especially common in more mathematical/statistical work as opposed to algorithmic work on clustering.

```{code-cell} ipython3
km = KMeans(n_clusters=3)
```

We dropped the *column* that tells us which of the three classes that each sample(row) belongs to.  We still have data from three species of flows.



```{hint}
use shift+tab or another jupyter help to figure out what the parameter names are for any function or class you're working with.
```

+++

Since we don't have separate test and train data, we can use the `fit_predict` method.  This is what the kmeans algorithm always does anyway, it both learns the means and the assignment (or prediction) for each sample at the same time. 

```{code-cell} ipython3
km.fit_predict(iris_X)
```


This gives the labeled cluster by index, or the assignment, of each point.

+++
If we run that a few times, we will see different solutions each time because the algorithm is random, or stochastic. 
+++

These are similar to the outputs in classification, except that in classification, it's able to tell us a specific species for each. Here it can only say clust 0, 1, or 2.  It can't match those groups to the species of flower.

+++

Now that we know what these are, we can save them to our DataFrame 
````{margin}
```{note}
I changed the column name here relative to class for clarity
```
````

```{code-cell} ipython3
iris_df['km3_1'] = km.fit_predict(iris_X)
```


## Visualizing the outputs

+++

Add the predictions as a new column to the original `iris_df` and make a `pairplot` with the points colored by what the clustering learned.

```{code-cell} ipython3
sns.pairplot(data=iris_df,hue='km3_1')
```


## Clustering Persistence

We can run kmeans a few more times and plot each time and/or compare with a neighbor/ another group.

```{code-cell} ipython3
iris_df['km3_2'] = km.fit_predict(iris_X)
```

We can use the `vars` parameter to plot only the measurement columns and not the cluster labels.  We didn't have to do this before, because `species` is strings, so seaborn knows to not plot it, but the cluster predictions are also numerical, so by default seaborn plots them.

```{code-cell} ipython3
measurement_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
sns.pairplot(data=iris_df,hue = 'km3_2', vars=measurement_cols)
```

```{code-cell} ipython3
iris_df['km3_3'] = km.fit_predict(iris_X)
measurement_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
sns.pairplot(data=iris_df,hue = 'km3_3', vars=measurement_cols)
```


We will use a loop to add a few more. 

````{margin}
```{tip}
using the i as a loop variable here makes sense since we're actually just repeating for the sake of repeating
```
````

```{code-cell} ipython3
for i in range(4,15):
    iris_df['km3_' +str(i)] = km.fit_predict(iris_X) 
```

If we look at the assignments, we can see how similar they are. 
```{code-cell} ipython3
iris_df[measurement_cols].sample(10)
```

Notice that while the numbers vary across the columns, the same rows have are the same or different across the columns.  This means that each sample sometimes gets assigned to a different label, but the same ones are grouped together. 

```{code-cell} ipython3
sns.pairplot(iris_df,hue='species',vars=measurement_cols)
```

```{code-cell} ipython3
iris_df.columns
```

The *grouping* of the points stay the same across different runs, but which color each group gets assigned to changes. Which blob is which color changes, but the partitions are basically the same.


Today, we saw that the clustering solution was pretty similar each time in terms of which points were grouped together, but the labeling of the groups (which one was each number) was different each time.  We also saw that clustering can only number the clusters, it can't match them with certainty to the species. This makes evaluating clustering somewhat different, so we need new metrics.

## Clustering Evaluation


$$ s = \frac{b-a}{max(a,b)}$$

+++
a: The mean distance between a sample and all other points in the same class.

b: The mean distance between a sample and all other points in the next nearest cluster.


This score computes a ratio of how close points are to points in the same cluster vs other clusters


We drew pictures in prismia, but we can also generate samples to make a plot that shows a bad silhouette score: 

```{code-cell} ipython3
:tags: ["hide-input"]
N = 100
bad_cluster_data = np.random.multivariate_normal([5,5],.75*np.eye(2), size=N)
data_cols = ['x0','x1']
df = pd.DataFrame(data=bad_cluster_data,columns=data_cols)
df['cluster'] = np.random.choice([0,1],N)

sns.relplot(data=df,x='x0',y='x1',hue='cluster',)
plt.title('S = ' + str(metrics.silhouette_score(df[data_cols],df['cluster'])));
```

And we can make a plot for a good silhouette score

```{code-cell} ipython3
:tags: ["hide-input"]
N = 50
single_cluster = np.random.multivariate_normal([1,1],
                                                .05*np.eye(2), 
                                                 size=N)
good_cluster_data = np.concatenate([single_cluster,50+single_cluster])
data_cols = ['x0','x1']
df = pd.DataFrame(data=good_cluster_data,columns=data_cols)
df['cluster'] = [0]*N + [1]*N

sns.relplot(data=df,x='x0',y='x1',hue='cluster',)
plt.title('S = ' + str(metrics.silhouette_score(df[data_cols],df['cluster'])));
```

Then we can use the silhouette score on our existing clustering solutions. 


```{code-cell} ipython3
metrics.silhouette_score(iris_X,iris_df['km3_2'])
```

We can also apply it using a lambda:

```{code-cell} ipython3
sil = lambda col: metrics.silhouette_score(iris_X, col)
iris_df[['km3_1', 'km3_2', 'km3_3', 'km3_4', 'km3_5', 'km3_6', 'km3_7', 'km3_8',
       'km3_9', 'km3_10', 'km3_11', 'km3_12', 'km3_13', 'km3_14']].apply(sil)
```


All of these solutions were the same

## Finding the right number of Clusters

When we do not know the right number, we can try different numbers and compare the silhouette score.

```{code-cell} ipython3
km2 = KMeans(n_clusters=2)
iris_df['km2'] = km2.fit_predict(iris_X)

km4 = KMeans(n_clusters=4)
iris_df['km4'] = km4.fit_predict(iris_X)
metrics.silhouette_score(iris_df[measurement_cols],iris_df['km2']), metrics.silhouette_score(iris_df[measurement_cols],iris_df['km4'])
```

For this datast, 2 clusters describes the data best, even though the dat came from 3 species of flowers. 
This is a common thing to observe.

While we sometims describe things as discrete, in nature a lot of things vary fairly continuously.  Clustering works best for things that are truly discrete, but can be useful even when it is not a perfct fit. 



## Mutual Information

When we know the truth, we can see if the learned clusters are related to the true groups, we can't compare them like accuracy but we can use a metric that is intuitively like a correlation for categorical variables, the mutual information.

The `adjusted_mutual_info_score` method in the `metrics` module computes a version of mutual information that is normalized to have good properties. 

```{code-cell} ipython3
metrics.adjusted_mutual_info_score(iris_df['species'],iris_df['km3_1'])
```

```{code-cell} ipython3
metrics.adjusted_mutual_info_score(iris_df['species'],iris_df['km3_2'])
```

## Questions After Class



### How do you know the right number of clusters to use before knowing anything about the data?

You do not.  You have to try it out.  Either like we did above by trying different numbers and checking the score, or using a clustering technique that can learn the number at the same time. 



### How to determine  which  cluster is what?

That you have to assign based on looking at the samples.  Remember in a *real* clustering situation, you do not know what the groups are. 

Typically, once we find the clusters, we look at the a few samples in each cluster to try to assign a name for it.  For example in th Etsy case of clustering product images to find "styles" after they have the groups of images, they look at a few that are near the "center" of the cluster for each cluster to come up with a name for each one.  

In the COPD example, after they found the groups, the doctors continued studying what was going on in each group.  They will do genetic analysis and continue studying to try to decide a name for each. 

### Difference between k-means, elbow, and silhouette

K-means is the clustering algorithm.  The elbow technique refers to plotting the score for  different solutions and then picking the one at that looks like an elbow or right before it to be your final thing. 
Silhoette is a score to evaluate clustering solutions. 
