---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Web Scraping

```{note}
Dates:

- A5 due 10/10
- P1 due 10/16 (ideally you have already started this)
- A6 due 10/18 (it's short, I promise)
- then back to assignments due Monday
```


```{code-cell} ipython3
import requests
from bs4 import BeautifulSoup
import pandas as pd
```

````{warning}
If it says it cannot load one of the libraries, use pip inside your notebook to install, 

```
pip install beautifulsoup4
```

then restart your kernel (Kernel menu, choose restart)
````


## Getting Data From Websites 


We have seen that `read_html` can get content from an actual website, not a data file that is hosted somewhere on the internet, that takes tables on a website and returns a list of DataFrames.

````{margin}
```{note}
This has a long output, so I hid it by default, but you can view it
```
````

```{code-cell} ipython3
:tags: ["hide-output"]
pd.read_html('https://rhodyprog4ds.github.io/BrownSpring23/syllabus/achievements.html')
```

This gives us a list of DataFrames that come from the website.  `pandas` gets tables by looking in the html for the site and finding the `<table>` tags. 


## Everything is Data

For the purpose of this class, it is best to think of the content on a web page like a datastructure.  

![html anatomy](https://cdn2.hubspot.net/hub/53/file-1519188549-jpg/blog-files/webpage-setup.jpg)


![HTML tree structure](http://www.w3schools.com/js/pic_htmltree.gif)

there are tags `<>` that define the structure, and these can be further classified with `classes`



## Scraping a URI website


We're going to create a DataFrame about URI CS & Statistics Faculty.

from the [people page](https://web.uri.edu/cs/people/) of the department website.

````{margin}
```{note}
the inspect link goes to instructions for different browsers
```
````
We can [inspect](https://blog.hubspot.com/website/how-to-inspect) the page to check that it's well structured.  

```{warning}
With great power comes great responsibility.

- always check the [robots.txt](https://web.uri.edu/robots.txt)
- do not do things that the owner says not to do
- government websites are typically safe
```


We'll save the URL for easy use



```{code-cell} ipython3
cs_people_url = 'https://web.uri.edu/cs/people/'
```

Then we can use the `requests` library to make a call to the internet.  It actually gets back a [response object](https://requests.readthedocs.io/en/latest/api/#requests.Response) which has a lot of extra information.  For today we only need the `content` from the page which is an attrtibute of that object: 

```{code-cell} ipython3
cs_people_html = requests.get(cs_people_url).content
```

This is raw: 
````{margin}
```{note}
here I suppressed th output in class by looking only at the first few characters
```
````
```{code-cell} ipython3
cs_people_html[:200]
```


But we do not need to manually write search tools, that's what [`BeautifulSoup`](https://beautiful-soup-4.readthedocs.io/en/latest/) is for.

```{code-cell} ipython3
cs_people = BeautifulSoup(cs_people_html,'html.parser')
```

```{code-cell} ipython3
type(cs_people)
```


### Looking at tags 

In this object we can use any tag from the file and get back the first instance

```{code-cell} ipython3
cs_people.a
```

We also see `<h3>` in the code so we can get the first one like this: 

```{code-cell} ipython3
cs_people.h3
```


this [cheatsheet](https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf) shows lots of html tags, but for this purpose you do not really need it. You'll be inspecting the page and then looking for what you want

### Searching the source


More helpful is the `find_all` method we wnat to find all `div` tags that are "peopleitem" class. We decided this by inspecting the code on the website.

```{code-cell} ipython3
type(cs_people.find_all('div','peopleitem'))
```

```{code-cell} ipython3
cs_people.find_all('div','peopleitem')
```


this is a long, object and we can see it looks iterable (`[` at the start)

```{code-cell} ipython3
people_items = cs_people.find_all('div','peopleitem')
len(people_items)
```



```{important}
answer to questions about searching [from the docs](https://beautiful-soup-4.readthedocs.io/en/latest/index.html?highlight=find_all#the-keyword-arguments)
```

We can also look at only the first instance

```{code-cell} ipython3
people_items[0]
```

We notice that the name is inside a `<h3>` tag with class `p-name` and then inside an a tag after that.  We also know from looking at the overall page that there are lots of other a tags, so we do not want to search all of those.
```{code-cell} ipython3
people_items[0].find('h3','p-name')
```

Then we see in this, there is an `<a>` tag, so we can pull that out next, we can use the tag attribute, because the first instance of the tag is exactly what we want. 

```{code-cell} ipython3
people_items[0].find('h3','p-name').a
```

inside that there is the text in a string, so we can pull that out 
```{code-cell} ipython3
people_items[0].find('h3','p-name').a.string
```

````{margin}
```{important}
In the course notes, I repeat very similar cells so that the notes replicate the *thinking* process to do the analysis. In your assignments, you do not need to include intermediate steps, there the target audience is a person interested in the conclusions where here, as students, *you* are the target audience. 
```
````

Finally, now that we know how to get one out, we can put it all in a list comprehension



```{code-cell} ipython3
names = [person.find('h3','p-name').a.string for person in people_items]
```

## Pulling more information

First, we look at the whole person entry again. 

```{code-cell} ipython3
people_items[0]
```
How to pull out the titles for each person (eg Assitatn Teaching Professor, Associate Professor)

on one item, the `p` tag with the `people-title` class gets us what we want and
then we can 

```{code-cell} ipython3
[person.find('p','people-title').a.string for person in people_items]
```

This give an error because there is no `<a>` tag inside the `<p>` tag

Python's null concept (outside of pandas and numpy that have nan float values) is `None` we can assign it to a variable if we want.  

```{code-cell} ipython3
a = None
```

Its type is `NoneType`
```{code-cell} ipython3
type(a)
```

So the error message says that the thing we are applying `.string` to is `None`, since that is the `.a` which means there is no `<a>` inside a `<p class = 'people-title'>`

If we take out the `a` it works and then we can iterate and store like above. 

```{code-cell} ipython3
titles = [person.find('p','people-title').string for person in people_items]
```

We can pull out two more things, the people-department indicates who is CS & who is Statistics.
```{code-cell} ipython3
disciplines = [d.string for d in cs_people.find_all("p",'people-department')]
emails = [e.string for e in cs_people.find_all("a",'u-email')]
```


We can finally use the DataFrame constructor to make it a table.  I chose to use a dictionary in class
```{code-cell} ipython3
css_df = pd.DataFrame({'name':names,'title':titles,'email':emails,'discipline':disciplines})
css_df
```

We could also use a list of list and separate list of column names. 
```{code-cell} ipython3
css_df_b = pd.DataFrame(data=[names,titles,emails,disciplines],
                       columns =['names','titles','emails','disciplines'])
css_df_b
```


## Crawling and scraping

Remember we pulled the names out of links, when in the browser, we click on the links, we see that they are to a profile page. On these pages, they have the office number.  Let's add those to our dataframe. 

First, we will do it for one person, then make a loop. 

```{code-cell} ipython3
people_items[0].find('h3','p-name').a
```

We see that the information that we want is in the `href` attribute, to read that, we check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#attributes). This tells us there is a `.attrs` attribute of the python object we are working with. 


```{code-cell} ipython3
people_items[0].find('h3','p-name').a.attrs
```

It's a dictionary and the attribute we want is the key we want. 

```{code-cell} ipython3
puggioni_url = people_items[0].find('h3','p-name').a.attrs['href']
```

Now, we do the same thing we did above, request, pull the content from the response and then use the parser. 

```{code-cell} ipython3
puggioni_html = requests.get(puggioni_url).content
puggioni_info = BeautifulSoup(puggioni_html,'html.parser')
```

then we find the tag and class we need from inspecting and pull that. 
```{code-cell} ipython3
puggioni_info.find_all('li','people-location')
```

it's an interable, so we pull the item out

```{code-cell} ipython3
puggioni_info.find_all('li','people-location')[0]
```

Then we try to pull the string out an that is empty
```{code-cell} ipython3
puggioni_info.find_all('li','people-location')[0].string
```

Here, we could go to the documentation and look up what the object contains, but insteas we can use object serialization.  
We can use the python `__dict__` to inspect the object and see where it stored what we want.  


```{code-cell} ipython3
puggioni_info.find_all('li','people-location')[0].__dict__
```


we see its the second element in a list in the `'content'` value
```{code-cell} ipython3
puggioni_info.find_all('li','people-location')[0].contents[1]
```

Now tht we know how to do it, we can put it in a loop.  

```{code-cell} ipython3
offices = []
for name_link in cs_people.find_all('h3','p-name'):
    url = name_link.a.attrs['href']
    person_html = requests.get(url).content
    person_info = BeautifulSoup(person_html,'html.parser')
    try: 
        offices.append(person_info.find_all('li','people-location')[0].contents[1])
    except:
        offices.append(pd.NA)


css_df['office'] = offices
```


We added the [`try` and `except`](https://docs.python.org/3/tutorial/errors.html#handling-exceptions) to handle when there is no office location. This is something in practice you would often think to do due an error.  

Here we check the `info` and we can see how it is. 


```{code-cell} ipython3
css_df.info()
```

We can also, finally save out our ready dataset: 

```{code-cell} ipython3
css_df.to_csv('css_faculty.csv')
```

```{code-cell} ipython3

```


## Questions after class

### what does .a do?
it gives the first instance of the `<a>` tag 

### is it worth it to try and web scrape a page that is poorly written?

If it is important information.  In these cases, you might have to do more manual parsing or even some manual fixes. 

For this class, no. 

### Is there a way to check robots.txt through BeautifulSoup or must that be done manually in a browser?

it could maybe be read programmaticlaly, but it doesn't necessarily save time to do it that way. 

### What else can I do with inspect?

It lets you view the code.  It's most often used to debug websites. 

### when web scraping if the html is not set up well is it possible to change the html to make it easier to parse

Technically you could manually edit a copy of it. 

### Are there instances where you can get data from websites that are not in tabular form?

Web scraping is *for* when the website is not in tabular form.  It should be strucutred, but the structure does not need to come from a single page.  It could be that there are many pages strucutred similarly and you build most of the columns from the other pages, not the starting page. 

For example from the [teams page of the nba](https://www.nba.com/teams) you can get to a page with info about each team that includes all time records and the current rosters. On these individual pages, most info is an actual table, so you can use `pd.read_html` for those, but the crawing part from the first page would count. 

### A source table would be the people's page on the URI website, but when you click on their individual names does that count as another source table?﻿

Not as we did above because we combined the data by adding another column.  If you built a whole table on each of the sub-pages it would count. 

## Portfolio Question


### I guess how to further edit the submission_1.intro. I know about the chapters you gotta add, but what else? Is submission_1.intro the only file you gotta edit?

Yes you edit that file and the `_toc.yml`.  There are instructions on the [portfolio page](../portfolio/index.md)

There are also [formatting tips](../portfolio/formatting.md) and [ideas](../portfolio/check1ideas.md)


### for portfolio one, can we submit whatever we want? like as little OR as much?

Yes, exactly! 

However, I do want to **really** encourage you to submit whatever you are thinking of, even if it is not as complete as you want.  If you submit, you will get feedback even if you do not earn all of the achievements you try. Tht will prepare you for the next one. 

