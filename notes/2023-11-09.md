---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---



# Model Optimization
Today we will learn how to find the best hyper parameter values for a model.

We have seen a couple hyperparameters so far:
- depth of the decision tree
- number of clusters in KMeans
  
but most models have some hyperparameters, or things that we can control to change how the the fit method works. 

```{code-cell}  ipython3 
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn import datasets
from sklearn import cluster
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn import tree
```

```{code-cell}  ipython3
:prismiaParentId: 624b0137-8236-41e4-96d9-f45b7f6d2113

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn import datasets
from sklearn import cluster
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn import tree
```



We will go back to our familiar iris data. 

Load it in from sklearn


now we will make training and test data
```{code-cell}  ipython3 
iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y)
```

```{code-cell}  ipython3 



iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y)
```



```{code-cell}  ipython3


svm_clf = svm.SVC(kernel='linear')
```


```{code-cell}  ipython3 
svm_clf.fit(iris_X_train,iris_y_train)
```

```{code-cell}  ipython3
svm_clf.fit(iris_X_train,iris_y_train)
```



```{code-cell}  ipython3 
svm_clf.score(iris_X_test, iris_y_test)
param_grid = {'kernel':['linear','rbf'], 'C':[.5, 1, 10]}
svm_opt = GridSearchCV(svm_clf,param_grid,)
```

```{code-cell}  ipython3
svm_clf.score(iris_X_test, iris_y_test)
param_grid = {'kernel':['linear','rbf'], 'C':[.5, 1, 10]}
svm_opt = GridSearchCV(svm_clf,param_grid,)
```



```{code-cell}  ipython3
svm_opt.fit(iris_X, iris_y)
```


```{code-cell}  ipython3
svm_opt.cv_results_
```

```{code-cell}  ipython3
:prismiaParentId: d052cde4-4dc1-46a7-bcb5-1bc8bfa4ed63

svm_opt.cv_results_
```


```{code-cell}  ipython3
pd.DataFrame(svm_opt.cv_results_)
```

```{code-cell}  ipython3
pd.DataFrame(svm_opt.cv_results_)
```


```{code-cell}  ipython3 
svm_opt.best_estimator_.predict(iris_X_test)
```

```{code-cell}  ipython3


svm_opt.best_estimator_.predict(iris_X_test)
```

+
## What does an SVM learn?


Find the optimal criterion, max_depth and min_samples_leaf for a decision tree on the iris data using GridSearchCV

```{code-cell}  ipython3
dt = tree.DecisionTreeClassifier()
params_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4],
             'min_samples_leaf':list(range(2,20,2))}
```

```{code-cell}  ipython3
dt = tree.DecisionTreeClassifier()
params_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4],
             'min_samples_leaf':list(range(2,20,2))}
```


```{code-cell}  ipython3
dt_opt = GridSearchCV(dt,params_dt)
dt_opt.fit(iris_X,iris_y)
dt_opt.best_params_
```

```{code-cell}  ipython3

dt_opt = GridSearchCV(dt,params_dt)
dt_opt.fit(iris_X,iris_y)
dt_opt.best_params_
```

