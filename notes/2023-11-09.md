---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec: {}
---



# Model Optimization
Today we will learn how to find the best hyper parameter values for a model.

We have seen a couple hyperparameters so far:
- depth of the decision tree
- number of clusters in KMeans
  
but most models have some hyperparameters, or things that we can control to change how the the fit method works. 

```null
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn import datasets
from sklearn import cluster
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn import tree
```

```{code-cell}
:prismiaParentId: 624b0137-8236-41e4-96d9-f45b7f6d2113

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
from sklearn import datasets
from sklearn import cluster
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn import tree
```

+++ {"prismiaId": "05fd80a3-2c8c-4c1c-a636-3bde9c4ea4ec"}

We will go back to our familiar iris data. 

Load it in from sklearn


now we will make training and test data
```null
iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y)
```

```{code-cell}


iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X,iris_y)
```



```{code-cell}
:prismiaParentId: 95e35143-755e-45ed-a869-ab78a5185c06

svm_clf = svm.SVC(kernel='linear')
```


```null
svm_clf.fit(iris_X_train,iris_y_train)
```

```{code-cell}
:prismiaParentId: 0492cfe4-43dc-44e8-9f4d-f914a220e725

svm_clf.fit(iris_X_train,iris_y_train)
```



```null
svm_clf.score(iris_X_test, iris_y_test)
param_grid = {'kernel':['linear','rbf'], 'C':[.5, 1, 10]}
svm_opt = GridSearchCV(svm_clf,param_grid,)
```

```{code-cell}


svm_clf.score(iris_X_test, iris_y_test)
param_grid = {'kernel':['linear','rbf'], 'C':[.5, 1, 10]}
svm_opt = GridSearchCV(svm_clf,param_grid,)
```



```{code-cell}
:prismiaParentId: a1a92265-9f12-435a-ae0c-c86c9b16a19f

svm_opt.fit(iris_X, iris_y)
```


```{code-cell}
svm_opt.cv_results_
```

```{code-cell}
:prismiaParentId: d052cde4-4dc1-46a7-bcb5-1bc8bfa4ed63

svm_opt.cv_results_
```


```{code-cell}
pd.DataFrame(svm_opt.cv_results_)
```

```{code-cell}
:prismiaParentId: 7dccebf8-29c4-4bc2-b597-29a214599991

pd.DataFrame(svm_opt.cv_results_)
```


```null
svm_opt.best_estimator_.predict(iris_X_test)
```

```{code-cell}
:prismiaParentId: 7715b20e-af90-49cd-b022-b4c18c2d9509

svm_opt.best_estimator_.predict(iris_X_test)
```

+
## What does an SVM learn?


Find the optimal criterion, max_depth and min_samples_leaf for a decision tree on the iris data using GridSearchCV

```{code-cell}
dt = tree.DecisionTreeClassifier()
params_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4],
             'min_samples_leaf':list(range(2,20,2))}
```

```{code-cell}
:prismiaParentId: 810527fa-f5d2-4d3e-a749-695247eaaee7

dt = tree.DecisionTreeClassifier()
params_dt = {'criterion':['gini','entropy'],'max_depth':[2,3,4],
             'min_samples_leaf':list(range(2,20,2))}
```


```{code-cell}
dt_opt = GridSearchCV(dt,params_dt)
dt_opt.fit(iris_X,iris_y)
dt_opt.best_params_
```

```{code-cell}

dt_opt = GridSearchCV(dt,params_dt)
dt_opt.fit(iris_X,iris_y)
dt_opt.best_params_
```

