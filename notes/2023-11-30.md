---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Neural Networks

## Admin
 
```{important}
We will have 2 speakers to wrap up the semester: 

- 12/7: [Nirmal Keshava](https://www.linkedin.com/in/nirmal-keshava-0180012/)
- 12/12: [Tiffany Sithiphone](https://www.linkedin.com/in/tiffanysithiphone/)
```

```{note}
P3 is due 12/4 unless you got a note from me saying that I saw your P2, but I have not graded it yet.
```

## What is a Neural Network


We started thinking about machine learning with the idea that the basic idea is
that we assume that our target variable ($y_i$) is related to the features $\mathbf{x}_i$
by some function (for sample $i$):

$$ y_i =f(\mathbf{x}_i;\theta)$$

But we don't know that function exactly, so we assume a type (a decision
  tree, a boundary for SVM, a probability distribution) that has some parameters
  $\theta$ and then use a machine
  learning algorithm $\mathcal{A}$ to estimate the parameters for $f$.  In the
  decision tree the parameters are the thresholds to compare to, in the GaussianNB the parameters are the mean and variance, in SVM it's the support vectors that define the margin.  

$$\theta = \mathcal{A}(X,y) $$

That we can use to test on our test data:

$$ \hat{y}_i = f(x_i;\theta) $$

A neural net allows us to not assume a specific form for $f$ first, it does
universal function approximation.  For one hidden layer and a binary classification problem:


$$f(x) = W_2g(W_1^T x +b_1) + b_2 $$

where the function $g$ is called the activation function. We approximate some
unknown, complicated function $f$ by taking a weighted sum of all of the inputs,
and passing those through another, known function, $g$.

The learning step involves finding the weights and biases (or coeffificents and intercepts). It does so by finding the weights that minimize some loss function on the data:

$$ min_{W_1,W_2,b_1,b_2} \ell(f(x),y)$$

where the loss function $\ell$ describes the "cost" of errors. For example it might be simple does the prediction match or more complex like how close it is.  



## NN in Sklearn

```{code-cell} ipython3
from scipy.special import expit
from sklearn.datasets import make_classification
from sklearn.neural_network import MLPClassifier
from sklearn import svm
import pandas as pd
import numpy as np


from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn import model_selection

from sklearn.model_selection import train_test_split

import seaborn as sns
sns.set_theme(palette='colorblind')
```


We will use the digits dataset again. 

```{code-cell} ipython3
digits = datasets.load_digits()
digits_X = digits.data
digits_y = digits.target
X_train, X_test, y_train, y_test = model_selection.train_test_split(digits_X,digits_y)
```

```{code-cell} ipython3
digits.images[0]
```


`sklearn` provides a neural network by the name `MLPClassifier`

```{code-cell} ipython3
mlp = MLPClassifier(
  hidden_layer_sizes=(16),
  max_iter=100,
  solver="lbfgs",
  verbose=10,
  random_state=1,
  learning_rate_init=0.1,
)
```

We specify:
- the number of neurons in each hidden layer
- the number of steps in the optimizaiton to do
- the solver is the algorithm used to find the parameters
- for it to output interim info as it works
- fix the random state so we all use the same initialization
- the initial learning rate (how fast to change parameter values while searching)


Then we use it just like we use all other sklearn estimators. 

```{code-cell} ipython3
mlp.fit(X_train, y_train).score(X_test,y_test)
```

## Comparing nn to mlp

Letsw fit an SVM

```{code-cell} ipython3
svm_clf = svm.SVC(gamma=0.001)
svm_clf.fit(X_train, y_train)
svm_clf.score(X_test,y_test)
```

Here we get better performance with this, but we can alos check the complexity to compare them. 

We can see how many support vectors that this had to store. 

```{code-cell} ipython3
svm_clf.support_vectors_.shape
```

and then multiply all together to get the total numbers stored: 

```{code-cell} ipython3
np.prod(list(svm_clf.support_vectors_.shape))
```

for the MLP, we'll use the weights: 

```{code-cell} ipython3
np.sum([np.prod(list(c.shape)) for c in mlp.coefs_])
```

We can see these shapes are determined by the data and the size of the hidden that we specifies and the number of classes. 

```{code-cell} ipython3
[list(c.shape) for c in mlp.coefs_]
```

In this case we have: 
- 64 features (8x8 pixels)
- 16 hidden layer neurons
- 10 classes 

we have 10 neurons in the output layer because each output neuron is related to one class. Each output neuron relates to one class, so for it to be one that class is predicted and the others are 0 for training.  At prediction time, we say the highest value one is the ones that we read as the prediction.  We interpret the outpus as the probability that the sample belongs to each class. 




## Neural Network Predictions

We'll start with some toy data for classification. 

```{code-cell} ipython3
X, y = make_classification(n_samples=100, random_state=1,n_features=2,n_redundant=0)
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y,
             random_state=1)
sns.scatterplot(x=X[:,0],y=X[:,1],hue=y)
```

it's two simple features. 


```{code-cell} ipython3
clf = MLPClassifier(
 hidden_layer_sizes=(1), # 1 hidden layer, 1 aritficial neuron
 max_iter=100, # maximum 100 interations in optimization
 alpha=1e-4, # regularization
 solver="lbfgs", #optimization algorithm  
 verbose=10, # how much detail to print
 activation= 'identity' # how to transform the hidden layer beofore passing it to the next layer
)
clf.fit(X_train, y_train)

clf.score(X_test, y_test)
```

this does very well

We can see that this network has one activation for the hidden layers 

```{code-cell} ipython3
clf.activation
```

and a different one for the output layer. 

```{code-cell} ipython3
clf.out_activation_
```


The sigmoid function looks like this: 

```{code-cell} ipython3
x_logistic = np.linspace(-10,10,100)
y_logistic = expit(x_logistic)
plt.plot(x_logistic,y_logistic)
```

The object also has coefficients
```{code-cell} ipython3
clf.coefs_
```

and intercepts as attributes. 
```{code-cell} ipython3
clf.intercepts_
```


To test this, we will make a new sample, the point (-1,2)
```{code-cell} ipython3
pt = np.array([[-1,2]])
```

The hidden neuron in this case does the following calculation: 

```{code-cell} ipython3
np.matmul(pt,clf.coefs_[0]) + clf.intercepts_[0]
```

then the output neuron takes that as input and uses its own weights, and the sigmoid function
```{code-cell} ipython3
expit((np.matmul(pt,clf.coefs_[0]) + clf.intercepts_[0])*clf.coefs_[1] +clf.intercepts_[1])
```

This calculates the probability the output is 1. 

```{code-cell} ipython3
clf.predict_proba(pt)
```

This method predicts the probabity of both 0 and 1. 

```{code-cell} ipython3
1- expit((np.matmul(pt,clf.coefs_[0]) + clf.intercepts_[0])*clf.coefs_[1] +clf.intercepts_[1])
```

and we can confirm that we have replicated the function of the predictions for the neural network. 


## Another way to replicate

We can consider a neuron like a tempalte function:

```{code-cell} ipython3
def aritificial_neuron_template(activation,weights,bias,inputs):
    '''
    simple artificial neuron

    Parameters
    ----------
    activation : function
    activation function of the neuron
    weights : numpy aray
    wights for summing inputs one per input
    bias: numpy array
    bias term added to the weighted sum
    inputs : numpy array
    input to the neuron, must be same size as weights

    '''
    return activation(np.matmul(inputs,weights) +bias)

# two common activation functions
identity_activation = lambda x: x
logistic_activation = lambda x: expit(x)
```

Notice that this function takes in: 
- inputs (features)
- weights
- bias
- activation function


We also define two activation functions. 

When we set up to train a neural network, we tell the learning algorithm what activation function to use and then it learns the weights. 


This is equivalent to our neural network above: 

```{code-cell} ipython3
hidden_neuron = lambda x: aritificial_neuron_template(identity_activation,clf.coefs_[0],clf.intercepts_[0],x)
output_neuron = lambda h: aritificial_neuron_template(expit,clf.coefs_[1],clf.intercepts_[1],h)

output_neuron(hidden_neuron(pt))
```

```{code-cell} ipython3

```


## A more complicated example

This time we'll make similar data with 4 features instead of 2  and we'll set up a test point `pt_4d`
```{code-cell} ipython3
X, y = make_classification(n_samples=200, random_state=1,n_features=4,n_redundant=0,n_informative=4)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,
                          random_state=5)
pt_4d =np.asarray([[-1,-2,2,-1],[1.5,0,.5,1]])
clf_4d = MLPClassifier(
  hidden_layer_sizes=(1),
  max_iter=5000,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  activation= 'identity'
)

clf_4d.fit(X_train, y_train)
clf_4d.score(X_test, y_test)
```

this does well again

we can see the data:


```{code-cell} ipython3
df = pd.DataFrame(X,columns=['x0','x1','x2','x3'])
df['y'] = y
sns.pairplot(df,hue='y')
```

we can do it again with our template function by defining two functions: one for the hidden neuron and one for output, then the prediction


```{code-cell} ipython3
hidden_neuron_4d = lambda x: aritificial_neuron_template(identity_activation,
                             clf_4d.coefs_[0],clf_4d.intercepts_[0],x)
output_neuron_4d = lambda x: aritificial_neuron_template(logistic_activation,
                             clf_4d.coefs_[1],clf_4d.intercepts_[1],x)


output_neuron_4d(hidden_neuron_4d(pt_4d))
```

```{code-cell} ipython3
clf_4d.predict_proba(pt_4d)
```

and confirm it's correct. 


```{code-cell} ipython3
pt_4d_2 =np.asarray([[-.5,-2,-1,-1],[1.5,0,-.5,1]])
output_neuron_4d(hidden_neuron_4d(pt_4d_2))
```

```{code-cell} ipython3
clf_4d.predict_proba(pt_4d_2)
```

We can build up what we need for a 4 hidden neuron MLP too.  First we'll train the MLP 


```{code-cell} ipython3
clf_4d_4h = MLPClassifier(
  hidden_layer_sizes=(4),
  max_iter=500,
  alpha=1e-4,
  solver="lbfgs",
  verbose=10,
  activation='logistic'
)

clf_4d_4h.fit(X_train, y_train)

```


Then use our function to build up the predictions
```{code-cell} ipython3
hidden_neuron_4d_h0 = lambda x: aritificial_neuron_template(logistic_activation,
                             clf_4d_4h.coefs_[0][:,0],clf_4d_4h.intercepts_[0][0],x)
hidden_neuron_4d_h1 = lambda x: aritificial_neuron_template(logistic_activation,
                             clf_4d_4h.coefs_[0][:,1],clf_4d_4h.intercepts_[0][1],x)
hidden_neuron_4d_h2 = lambda x: aritificial_neuron_template(logistic_activation,
                             clf_4d_4h.coefs_[0][:,2],clf_4d_4h.intercepts_[0][2],x)
hidden_neuron_4d_h3 = lambda x: aritificial_neuron_template(logistic_activation,
                             clf_4d_4h.coefs_[0][:,3],clf_4d_4h.intercepts_[0][3],x)
output_neuron_4d_4h = lambda x: aritificial_neuron_template(logistic_activation,
                             clf_4d_4h.coefs_[1],clf_4d_4h.intercepts_[1],x)
```

and finally call it all togther
```{code-cell} ipython3
output_neuron_4d_4h(np.asarray([hidden_neuron_4d_h0(pt_4d),
         hidden_neuron_4d_h1(pt_4d),
         hidden_neuron_4d_h2(pt_4d),
         hidden_neuron_4d_h3(pt_4d)]).T)
```

and compare with the MLP's own predications

```{code-cell} ipython3
clf_4d_4h.predict_proba(pt_4d)
```

## Questions 


### Are there neural networks wherein each layer does a different type of transformation, such as logistic or identity?

There are different types of layers and some are defined by activations, others are more complex calculations in other ways.

### What are the benefits of neural networks compared to machine learning? 

Neural networks are one type of machine learning model. 

### What is the larger contributing in advancements in deep learning - hardware or software?

Hardware advances were essential for 

### Are there any other visualizations of neural networks such as images,articles,videos that would be good for introductions? I would like some that do go into some of the math behind how it works.

This [free textbook](https://www.deeplearningbook.org/) is a good source by leaders in the feild. 

### Are you able to keep training data from a previous session with deep learning?

For any machine learning algorithm you can save the object or serialize the parameters to a file and then load them back in.  

For deep learning you can save the weight matrices and you can also then reinstantiate the object.  


[Hugging face](https://huggingface.co/models) contains pretrained models. 

### What other functions can we use instead of explit (sigmoid)?

The most common other one is RELU, we'll see more about that next week. 