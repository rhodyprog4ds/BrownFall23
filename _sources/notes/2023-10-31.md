---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Regression


## Feedback

See prismia notes for feedback qualitative comments

## Setting up for regression


We're going to predict **tip** from **total bill**.
This is a regression problem because the target, *tip* is:
 1. available in the data (makes it supervised) and 
 2.  a continuous value.
The problems we've seen so far were all classification, species of iris and the
character in that corners data were both categorical.  

Using linear regression is also a good choice because it makes sense that the tip
would be approximately linearly related to the total bill, most people pick some
percentage of the total bill.  If we our prior knowledge was that people
typically tipped with some more complicated function, this would not be a good
model.

```{code-cell} ipython3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
sns.set_theme(font_scale=2,palette='colorblind')
```




We wil load this data from `seaborn`
```{code-cell} ipython3
tips_df = sns.load_dataset('tips')
tips_df.head()
```

Since we will use only one variable, we have to do some extra work.  When we use sklearn with a DataFrame, it picks out the values, like this: 
```{code-cell} ipython3
tips_df['total_bill'].values
```

This is because originally sklearn actualy only worked on `numpy` arrays and sot what they do now is pull it out of the DataFrame, it gives us a numpy array:

```{code-cell} ipython3
type(tips_df['total_bill'].values)
```

All sklearn estimator objects are designed to work on multiple features, which mean they expect that the features have a 2D shape, but when we pick out a single column's (pandas Series) values, we get a 1D shape: 
```{code-cell} ipython3
tips_df['total_bill'].values.shape
```

We can change this by adding an axis.  It doesn't change values, but changes the way it is represented enough that it has the properties we need. 


```{code-cell} ipython3
tips_X = tips_df['total_bill'].values[:,np.newaxis]
```

This has the shape we want:

```{code-cell} ipython3
tips_X.shape
```

and is still the same type
```{code-cell} ipython3
type(tips_X)
```


The target is expected to be a single feature, so we do not need to do anythign special. 
```{code-cell} ipython3
tips_y = tips_df['tip']
```


Next, we split the data
```{code-cell} ipython3
tips_X_train, tips_X_test, tips_y_train, tips_y_test = train_test_split(tips_X,tips_y)
```

## Fitting a regression model

We instantiate the object as we do with all other sklearn estimator objects. 

```{code-cell} ipython3
regr = linear_model.LinearRegression()
```

and `fit` liek the others too. 

```{code-cell} ipython3
regr.fit(tips_X_train, tips_y_train)
```

we also can predict as before

```{code-cell} ipython3
tips_y_pred = regr.predict(tips_X_test)
```



## Regression Predictions

Linear regression is making the assumption that the target is a linear function of the features so that we can use the equation for a line(for scalar variables): 

$$ y =mx + b$$ 

becomes equivalent to the following code assuming they were all vectors/matrices

```
i = 1 # could be any number from 0 ... len(target)
target[i] = regr.coef_*features[i]+ regr.intercept_
```

You can match these up one for one. 
-  `target` is $y$ (this is why we usually put the `_y_` in the varaible name)
-  `regr.coef_` is the slope, $m$
-  `features` are the $x$ (like for y, we label that way)
-  adn `regr.intercept_` is the $b$ or the y intercept. 

```{important}
I have tried to expand the detail here substantively. Use the link at the top of the page to make an issue if you have specific questions
```


We will look at each of them and store them to variables here


```{code-cell} ipython3
slope = regr.coef_
slope
```

```{code-cell} ipython3
intercept = regr.intercept_
intercept
```

```{code-cell} ipython3
:tags: ["hide-cell"]
# extra import so that I can use the values of variables from the code outside of the 
# code cells on the website 
from myst_nb import glue

# "gluing" these variables for use
glue('slope_pct',slope*100)
glue('intercept',intercept)
glue('slope_rnd',np.round(slope*100))
glue('intercept_rnd',np.round(intercept*100)/100)
```

```{important}
This is what our model *predicts* the tip will be based on the past data.  It is
important to note that this is not what the tip *should* be by any sort of
virtues. For example, a typical normative rule for tipping is to tip 15% or 20%.
the model we learned, from this data, however is {glue:}`slope_pct`% + ${glue:}`intercept`. 
```

now we can pull out our first $x$ and calculate a predicted value of $y$ manually
```{code-cell} ipython3
slope*tips_X_test[0] + intercept
```

and see that this matches exactly the prediction from the `predict` method. 

```{code-cell} ipython3
tips_y_pred[0]
```

```{important}
Note that my random seed is not fixed, so the values there are going to change each time the
website is updated, but that these values will *always* be true
```````

## Visualizing Regression

Since we only have one feature, we can visualize what was learned here.  We will use plain
`matplotlib` plots because we are plotting from numpy arrays not data frames. 

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.scatter(tips_X_test,tips_y_pred, color='blue')
```

This plots the predictions in blue vs the real data in black.  The above uses them both as scatter plots to make it more clear, below, like in class, I'll use a normally inconvenient aspect of a line plot to show all the predictions the model could make in this range.  


```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test, color='black')
plt.plot(tips_X_test,tips_y_pred, color='blue')
```

Since the predictions are prefectly in a line, if I use a line plot instead of scatter to connect them all, it makes a line. 


## Evaluating Regression - Mean Squared Error

From the plot, we can see that there is some error for each point, so accuracy
that we've been using, won't work.  One idea is to look at how much error there
is in each prediction, we can look at that visually first.



These red lines are the {term}`residuals`, or errors in each prediction. 


In this code block, I use `zip` a 
[builtin function in Python](https://docs.python.org/3/library/functions.html#zip) 
to iterate over all of the test samples and predictions (they're all the same length) 
and plot each red line in a list comprehension.  This could have been a for loop, but
the comprehension is slighly more compact visually.  The zip allows us to have a Pythonic,
easy to read loop that iterates over multiple variables.  

To mape a vertical line, I make a line plot with just two values.  My plotted data is:
`[x,x],[yp,yt]`  so the first point is `(x,yp)` and the second is `x,yt`.  

The `;` at the end of each line suppressed the text output. 


```{code-cell} ipython3
# plot line prediction
plt.plot(tips_X_test, tips_y_pred, color='blue', linewidth=3);

# draw vertical lines from each data point to its predict value
[plt.plot([x,x],[yp,yt], color='red', linewidth=3, markevery=[0], marker ='^')
                 for x, yp, yt in zip(tips_X_test, tips_y_pred,tips_y_test)];

# plot these last so they are visually on top
plt.scatter(tips_X_test, tips_y_test,  color='black');
```
From this we can see that the errors are relatively small, which matches what we saw with the
mean squared error.  


To see mroe about how the code above works: 
```{code-cell} ipython3
:tags: ["hide-cell"]
plt.plot(tips_X_test, tips_y_pred, color='blue', linewidth=3);

# draw vertical lines from each data point to its predict value
for i in range(len(tips_X_test)):
    x = tips_X_test[i]
    yp = y_pred[i]
    yt = tips_y_test[i]
    plt.plot([x,x],[yp,yt], color='red', linewidth=3, markevery=[0], marker ='^')
                 
plt.scatter(tips_X_test, tips_y_test,  color='black');
```



We can use the average length of these red lines to capture the error. To get
the length, we can take the difference between the prediction and the data for
each point. Some would be positive and others negative, so we will square each
one then take the average.  

```{code-cell} ipython3
mean_squared_error(tips_y_test,tips_y_pred)
```

Which is equivalent to: 
```{code-cell} ipython3
np.mean((tips_y_test-tips_y_pred)**2)
```

To interpret this, we can take the square root to get it back into dollars. This becomes equivalent to taking the mean absolute value of the error. 

```{code-cell} ipython3
mean_abs_error = np.sqrt(mean_squared_error(tips_y_test,tips_y_pred))
mean_abs_error
```

```{code-cell} ipython3
np.mean(np.abs(tips_y_test-tips_y_pred))
```

Still, to know if this is a big or small error, we have to compare it to the values
we were predicting

```{code-cell} ipython3
tips_y_test.describe()
```

```{code-cell} ipython3
avg_tip = tips_y_test.mean()
tip_var = tips_y_test.var()
avg_tip, tip_var
```


```{code-cell} ipython3
:tags: ["hide-cell"]
glue('mae',mean_abs_error)
glue('avtip',avg_tip)
```

We can see that the error is smaller than most of the tips. 


(r2score_intuition)=
## Evaluating Regression - R2

We can also use the $R^2$ score, the [coefficient of determination](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score).

If we have the following:
- $n$ `=len(y_test)``
- $y$ `=y_test`
- $y_i$ `=y_test[i]`
- $\hat{y}$ = `y_pred`
- $\bar{y} = \frac{1}{n}\sum_{i=0}^n y_i$ = `sum(y_test)/len(y_test)`

$$ R^2(y, \hat{y}) = 1 = \frac{\sum_{i=0}^n (y_i - \hat{y}_i)^2}{\sum_{i=0}^n (y_i - \bar{y}_i)^2} $$ 


```{code-cell} ipython3
r2_score(tips_y_test, tips_y_pred)
```


This is a bit harder to interpret, but we can use some additional plots to
visualize.
This code simulates data by randomly picking 20 points, spreading them out
and makes the “predicted” y values by picking a slope of 3. Then I simulated various levels of noise, by sampling noise and multiplying the same noise vector by different scales and adding all of those to a data frame with the column name the r score for if that column of target values was the truth.

Then I added some columns of y values that were with different slopes and different functions of x. These all have the small amount of noise.

````{margin}
```{tip}
[Facet Grids](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html) allow more customization than the figure level plotting functions
we have used otherwise, but each of those combines a FacetGrid with a
particular type of plot.
```
````

```{code-cell} ipython3
# pick some random points
x = 10*np.random.random(20)
# make a line with a fixed slope
y_pred = 3*x
# put it in a data frame
ex_df = pd.DataFrame(data = x,columns = ['x'])
ex_df['y_pred'] = y_pred
# set some amounts of noise
n_levels = range(1,18,2)
# sample random noies
noise = (np.random.random(20)-.5)*2
# add the noise and put it in the data frame with col title the r2
for n in n_levels:
    y_true = y_pred + n* noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true

# set some other functions
f_x_list = [2*x,3.5*x,.5*x**2, .03*x**3, 10*np.sin(x)+x*3,3*np.log(x**2)]
# add them to the noise and store with r2 col title
for fx in f_x_list:
    y_true = fx + noise
    ex_df['r2 = '+ str(np.round(r2_score(y_pred,y_true),3))] = y_true    

# make the data tidy for plotting
xy_df = ex_df.melt(id_vars=['x','y_pred'],var_name='rscore',value_name='y')
# make a custom grid by using facet grid directly
g = sns.FacetGrid(data = xy_df,col='rscore',col_wrap=3,aspect=1.5,height=3)
# plot the lin
g.map(plt.plot, 'x','y_pred',color='k')
# plot the dta
g.map(sns.scatterplot, "x", "y",)
```


```{code-cell} ipython3
r2_score(tips_y_test,tips_y_pred)
```

This can help us interpret the score on our own too. 

```{code-cell} ipython3
mean_squared_error(tips_y_test,tips_y_pred)
```



```{code-cell} ipython3
regr.score(tips_X_test,tips_y_test)
```

```{code-cell} ipython3

```

## Questions after class

### How do you know if a regression model is good or works for a dataset? 

Let's break this down.  Regression is supervised learning, so there needs to be variables that you can use as features and one to use as the target. To test if supervised learning makes sense, try filling column names in place of `features` and `target` in the following sentence, that describes a prediction task: we want to predict `target` from `features`.  

Regression, specifically means that the target variable needs to be continuous, not discrete. For example, in today's class, we predicted the tip in dollars, which is continous in this sense.  This is not stictly continuous in the mathematical sense, but anything that is most useful to consider as continuous. 


_logisitic regression is actually a classification model despite the name_

### So is regression is just used with numeric values in terms of predicting?

Yes linear regression requires numeric values for the features as well as the target.  

Other types of regression can use other types of features. 

All regression is for a continusous target. 

### What are the strengths of a regression model as opposed to the previous models we've seen?

Using regression, classification, or clustering is based on what type of problem you are solving. 

Our simple flowchart for what task you are doing is:

```{mermaid}
flowchart TD
    labels{Do you have labels <br>in the dataset? }
    labels --> |yes| sup[Supervised Learning]
    labels --> |no| unsup[Unsupervised Learning]
    sup --> ltype{What type of variable <br> are the labels?}
    ltype --> |continuous| reg[Regression]
    ltype --> |discrete| clas[Classification]
    unsup --> groups{Do you think there are <br> groups within the data?}
    groups --> |yes | clus[Clutering]
```

the [sklearn docs include a more detailed guide](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

### Are linear models exclusively used for regression problems?

No, there are also linear classification models. 

### What does a negative R2 score mean?

It's a bad fit. 

### What is the difference between a good and bad rscore?

1 is perfect, low is bad. What is "good enough" is context dependent. 

### why were your values coming out negative?

Mine was not a very good fit.  We'll see why on Thursday.  Yours may have been better

### What would be a common example of a real-world linear regression?

Lots of real models are linear regression, but with many inputs.  It is often not a perfect fit, but good enough. 

### what are the coefficient and intercept ?

The coefficient is the slope of the line and the intercept is the value of the label the model predicts for features =0. 

### how do you add string data thats not binary e.g 'True' 'False' to make it fit in data?

You have to transform it to be numerical.  If it is True and False, you can cast it to integers. 

I recommend for A9 filtering on the UCI website to pick out data with numerical features. 

In general, and you can for A9 if you want, use the [sklearn `LabelEncodeer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to transform the data.  