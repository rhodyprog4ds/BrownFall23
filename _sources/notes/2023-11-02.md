---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Interpretting Regression

```{code-cell} ipython3
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
sns.set_theme(font_scale=2,palette='colorblind')
```

## Review - Univariate regression
(see previous class notes here)
```{code-cell} ipython3
tips_df = sns.load_dataset('tips')
tips_df.head()
```

```{code-cell} ipython3
tips_X = tips_df['total_bill'].values[:,np.newaxis]
tips_y = tips_df['tip']
tips_X_train, tips_X_test, tips_y_train,tips_y_test = train_test_split(tips_X,tips_y, random_state=5,train_size=.8)
```

```{code-cell} ipython3
regr = linear_model.LinearRegression()
regr.fit(tips_X_train,tips_y_train)
```

and get a score

```{code-cell} ipython3
regr.score(tips_X_test,tips_y_test)
```

```{code-cell} ipython3
regr.coef_
```

```{code-cell} ipython3
regr.intercept_
```

```{code-cell} ipython3
tips_y_pred = regr.predict(tips_X_test)
```

## Examining residuals more closely 

We can plot the residuals, or errors directly versus the input. 

```{code-cell} ipython3
plt.scatter(tips_X_test,tips_y_test-tips_y_pred,)
```

If our model was predicting uniformly well, these would be evenly distributed left to right. 


## Mutlivariate Regression

We can look at the estimator again and see what it learned. It describes the model like a line:

$$ \hat{y} = mx+b$$

except in this case it's multivariate, so we can write it like:

$$ \hat{y} = \beta^Tx + \beta_0 $$

where $\beta$ is the `regr_db.coef_` and $\beta_0$ is `regr_db.intercept_` and that's a vector multiplication and $\hat{y}$ is `y_pred` and $y$ is `y_test`.  

In scalar form it can be written like

$$ \hat{y} = \sum_{k=0}^d(x_k*\beta_k) + \beta_0$$

where there are $d$ features, that is $d$= `len(X_test[k])` and $k$ indexed into it. For example in the below $k=0$



```{code-cell} ipython3
tips_X = tips_df[['total_bill','size']]
tips_y = tips_df['tip']
tips_X_train, tips_X_test, tips_y_train,tips_y_test = train_test_split(tips_X,tips_y, random_state=5,train_size=.8)
```

We do not need the newaxis once we have more than one feature


```{code-cell} ipython3
tips_df[['total_bill','size']].values.shape
```

```{code-cell} ipython3
tips_df[['total_bill','size']].values[:,np.newaxis].shape
```

```{code-cell} ipython3

regr = linear_model.LinearRegression()
regr.fit(tips_X_train,tips_y_train)
```

```{code-cell} ipython3
tips_y_pred_2 = regr.predict(tips_X_test)
```

```{code-cell} ipython3
regr.score(tips_X_test,tips_y_test)
```

```{code-cell} ipython3
tips_y_pred_2.shape, tips_y_test.shape,tips_X_test.shape
```

```{code-cell} ipython3
plt.scatter(tips_X_test['total_bill'],tips_y_pred_2-tips_y_test)
```

```{code-cell} ipython3
plt.scatter(tips_X_test['size'],tips_y_pred_2-tips_y_test)
```

```{code-cell} ipython3
regr.coef_
```

```{code-cell} ipython3
regr.intercept_
```

```{code-cell} ipython3
tips_X_test.head()
```

```{code-cell} ipython3
np.sum(tips_X_test.values[0]*regr.coef_) +regr.intercept_
```

```{code-cell} ipython3
tips_X_test.values[0][0]*regr.coef_[0] + tips_X_test.values[0][1]*regr.coef_[1] +regr.intercept_
```

```{code-cell} ipython3
tips_y_pred_2[0]
```

```{code-cell} ipython3
from sklearn.preprocessing import PolynomialFeatures
```

```{code-cell} ipython3
poly = PolynomialFeatures()
```

```{code-cell} ipython3

tips_X2_train = poly.fit_transform(tips_X_train)
```

```{code-cell} ipython3
regr2 = linear_model.LinearRegression()
regr2.fit(tips_X2_train,tips_y_train)
```

```{code-cell} ipython3
tips_X2_test = poly.fit_transform(tips_X_test)
tips_y_pred_sq = regr2.predict(tips_X2_test)
```

```{code-cell} ipython3
regr2.score(tips_X2_test,tips_y_test)
```

## Questions After Class 

```{note}
most were something like re-explain what just happened, so I did not answer them separately below, but made sure to add in detail above
```

<!-- ### i am familiar with the concept of linear transformations hwoever, can you please go over how this would apply to real examples of data and what that would help us achieve by doing a linear transformation?

This is applicable whenever you think there might be a quadratic relationship in the data. 

The practical advantage is that we do not need to implement specialized estimator objects for every type of regression.  

In practice, we an transform the feeatures any way we want.  It does not need to be only a linear transformation.  -->

### where can I find good datasets for practice?

The UCI repository is the best place to start. Use the [filters](https://archive.ics.uci.edu/datasets?Task=Regression&NumInstances=100-1000&FeatureTypes=Numerical&skip=0&take=10&sort=desc&orderBy=NumHits&search=) to select for regression, of moderate size, with numerical features.  

You could also use a dataset with both numerical and categorical features and only us the numerical ones.  Remember how in A2, I had you subset data to select for numerical features?  There was a goal to that. 

### Does making the data quadratic help to find quadratic patterns, or does it simply give the model more data?

It finds quadratic patterns. That model has the same number of samples and, in a sense, the same amount of "data" but it has sort of a different view on the same data.  Computing functions of our data is not considered to be generating *more* data, it's thought of as a change of representation only. 

When we added a second variable to the model, that was giving it more information about each sample and actually increasing the size of the data. 

### How specifically does the poly.fit_transform work?

For the details of the code, you can see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).  From there you can see the [source for the `PolynomialFeatures` class](https://github.com/scikit-learn/scikit-learn/blob/093e0cf14/sklearn/preprocessing/_polynomial.py#L98). That inherits from the [`TransformerMixin`](https://github.com/scikit-learn/scikit-learn/blob/093e0cf14aff026cca6097e8c42f83b735d26358/sklearn/base.py#L876) which defines the [`fit_transform` method](https://github.com/scikit-learn/scikit-learn/blob/093e0cf14aff026cca6097e8c42f83b735d26358/sklearn/base.py#L888) which calls, in sequence [the `fit` and `transform` methods](https://github.com/scikit-learn/scikit-learn/blob/093e0cf14aff026cca6097e8c42f83b735d26358/sklearn/base.py#L916) as they are defined in the Polynomial features class. In this case the fit calculates the number of total output features and transform transforms the input features and outputs the new features.  

Mathematically, we provide it a matrix, $X\in \mathcal{R}^{n\times d}$ where the columns are vectors $\mathbf{x_i}\in  \mathcal{R}^{n}$ for $i\in [0,\ldots,d]$ which means that there are $d$ columns representing the original features and $n$ samples.  We get back a new matrix with columns: $[1, x_0, x_0^2, x_0*x_1,x_0*x_2, \ldots,x_0*x_d, x_1^2, x_1*x_2, \ldots, x_1*x_d,\ldots x_d^2]$

This means that we can use the same algorithm to find the weights as we do for linear regression, using the transformed features.  

A linear regression solver can sovle for the coefficients no matter what they are multiplied by. 

### Can you further explain the Y = B1x1 + B2x1^2 + B0 for both single variables and multiple variables?

### I think I would just like another run down of the equation or maybe even just written in explanations