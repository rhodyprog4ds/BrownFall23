---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Tidy Data and Structural Repairs


## Intro

This week, we'll be cleaning data.

Cleaning data is **labor intensive** and requires making *subjective* choices.  
We'll focus on, and assess you on, manipulating data correctly, making reasonable
choices, and documenting the choices you make carefully.

We'll focus on the programming tools that get used in cleaning data in class
this week:
- reshaping data
- handling missing or incorrect values
- renaming columns


```{warning}
this is incomplete, but will get filled in this week. 
```

```{code-cell} ipython3
import pandas as pd
import seaborn as sns

# make plots look nicer and increase font size
sns.set_theme(font_scale=2, palette='colorblind')
```

## What is Tidy Data

Read in the three csv files described below and store them in a list of dataFrames
```{code-cell} ipython3
url_base = 'https://raw.githubusercontent.com/rhodyprog4ds/rhodyds/main/data/'

datasets = ['study_a.csv','study_b.csv','study_c.csv']
```

```{code-cell} ipython3
study_df_list = [pd.read_csv(url_base +cur_study,na_values='-') 
                 for cur_study in datasets]
```

```{code-cell} ipython3
study_df_list[0]
```

```{code-cell} ipython3
study_df_list[1]
```

```{code-cell} ipython3
study_df_list[2]
```


These three all show the same data, but let's say we have two goals:
- find the average  effect per person across treatments
- find the average effect per treatment across people

This works differently for these three versions.


```{code-cell} ipython3
df_a = study_df_list[0]
df_a.mean(numeric_only=True,)
```

we get the average per treatment, but to get the average per person, we have to go across rows, which we can do here, but doesn't work as well with plotting

we can work across rows with the `axis` parameter if needed

```{code-cell} ipython3
df_a.mean(numeric_only=True,axis=1)
```

```{code-cell} ipython3
df_b = study_df_list[1]
df_b
```

Now we get the average per person, but what about per treatment? again we have to go across rows instead.

```{code-cell} ipython3
study_df_list[2]
```


For the third one, however, we can use groupby, because this one is tidy. 

```{code-cell} ipython3
study_df_list[2].groupby('treatment').mean(numeric_only=True)
```

```{code-cell} ipython3
study_df_list[2].groupby('person').mean(numeric_only=True)
```

The original [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) paper is worth reading to build a deeper understanding of these ideas.  


## Tidying Data


Let's reshape the first one to match the tidy one. First, we
will save it to a DataFrame, this makes things easier to read
and enables us to use the built in help in jupyter, because it can't check types too many levels into a data structure.

```{code-cell} ipython3
df_a.melt(id_vars='name', value_vars=['treatmenta','treatmentb'])
```


When we melt a dataset:
- the `id_vars` stay as columns
- the data from the `value_vars` columns become the values in the `value` column
- the column names from the `value_vars` become the values in the `variable` column
- we can rename the value and the variable columns.

```{code-cell} ipython3
df_a.melt(id_vars='name', value_vars=['treatmenta','treatmentb'],
          var_name= 'treatment_type',
          value_name='result',)
```

## Transforming the Coffee Data

Let's do it for our coffee data:

```{code-cell} ipython3
arabica_data_url = 'https://raw.githubusercontent.com/jldbc/coffee-quality-database/master/data/arabica_data_cleaned.csv'
# load the data
coffee_df = pd.read_csv(arabica_data_url, index_col=0)
```

```{code-cell} ipython3
coffee_df.head(2)
```

```{code-cell} ipython3
coffee_df.columns
```

```{code-cell} ipython3
scores_of_interest = ['Aroma',
       'Flavor', 'Aftertaste', 'Acidity', 'Body', 'Balance', 'Uniformity',
       'Clean.Cup', 'Sweetness',]
```

```{code-cell} ipython3
coffee_df[scores_of_interest].head(2)
```

We can make this tall using melt to transform with that set of questions and then rename the `value` and `variable` columns to be descriptive. 

```{code-cell} ipython3
coffee_df_tall = coffee_df.melt(id_vars='Country.of.Origin',value_vars=scores_of_interest,
               var_name='rating_type',
               value_name='score')
coffee_df_tall.head(1)
```

Notice that the actual column names inside the `scores_of_interest` variable become the values in the variable column.

This one we can plot in more ways:

```{code-cell} ipython3
sns.displot(data = coffee_df_tall, kind = 'kde', x='score',
            col='rating_type', hue='Country.of.Origin',col_wrap=3)
```

find why warning

```{code-cell} ipython3

```


## Filtering Data

```{warning}
this was not in class this week, but is added here for completeness
```

```{code-cell} ipython3
high_prod = coffee_df[coffee_df['Number.of.Bags']>250]
high_prod.shape
```

```{code-cell} ipython3
coffee_df.shape
```

We see that filters and reduces.  We can use any boolean expression in the square brackets.

```{code-cell} ipython3
top_balance = coffee_df[coffee_df['Balance']>coffee_df['Balance'].quantile(.75)]
top_balance.shape
```
We can confirm that we got only the top 25% of balance scores:
```{code-cell} ipython3
top_balance.describe()
```


We can also use the `isin` method to filter by comparing to an iterable type

```{code-cell} ipython3
total_per_country = coffee_df.groupby('Country.of.Origin')['Number.of.Bags'].sum()
top_countries = total_per_country.sort_values(ascending=False)[:10].index
top_coffee_df = coffee_df[coffee_df['Country.of.Origin'].isin(top_countries)]
```


## More manipulations

```{warning}
this was not in class this week, but is added here for completeness
```

Here, we will make a tiny `DataFrame` from scratch to illustrate a couple of points

```{code-cell} ipython3
large_num_df = pd.DataFrame(data= [[730000000,392000000,580200000],
                                   [315040009,580000000,967290000]],
                           columns = ['a','b','c'])
large_num_df
```

This dataet is not tidy, but making it this way was faster to set it up.  We could make it tidy using melt as is. 

```{code-cell} ipython3
large_num_df.melt()
```

However, I want an additional variable, so I wil reset the index, which adds an index column for the original index and adds a new index that is numerical. In this case they're the same.   

```{code-cell} ipython3
large_num_df.reset_index()
```

If I melt this one, using the index as the `id`, then I get a reasonable tidy DataFrame

```{code-cell} ipython3
ls_tall_df = large_num_df.reset_index().melt(id_vars='index')
ls_tall_df
```

Now, we can plot.

```{code-cell} ipython3
sns.catplot(data = ls_tall_df,x='variable',y='value',
            hue='index',kind='bar')
```

Since the numbers are so big, this might be hard to interpret.  Displaying it with all the 0s would not be easier to read.  The best thing to do is to add a new colum with adjusted values and a corresponding title.

```{code-cell} ipython3
ls_tall_df['value (millions)'] = ls_tall_df['value']/1000000
ls_tall_df.head()
```

Now we can plot again, with the smaller values and an updated axis label.  Adding a column with the adjusted title is good practice because it does not lose any data and since we set the value and the title at the same time it keeps it clear what the values are. 



## Questions after Class


### In the data we had about treatment a and treatment b.  Say there was another column that provided information about the age of the people.  Could we create another value variable to or would we put it in the value_vars list along with treatment a and treatmentb?
We can have multiple variables as the `id_vars` so that the dataset would have 4 columns instead of 3.



### Are there any prebuilt methods to identify and remove extreme outliers?

There may be, that is a good thing to look in the documentation for.  However, typically the definition of an outlier is best made within context, so the filtering strategy that we just used would be the way to remove them, after doign some EDA.


### Is there a specific metric to which a dataset must meet to consider it 'cleaned'?
It's "clean" when it will work for the analysis you want to do. This means clean enough for one analysis may not be enough for another goal.  Domain expertise is alwasy important.



### how does melt work underneath the hood?

The [melt documentation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) is the place to read about the details. It also includes alink to the [source code](https://github.com/pandas-dev/pandas/blob/v2.1.1/pandas/core/reshape/melt.py#L34-L151) if reading how it is implemented in code is more helpful to you can reading English. 

### Do people ever write their melted datasets to new files for redistribution, maybe to make things easier for future researchers?

Yes! If you look in the R Tidy Tuesday datsets most distribute the "raw" and cleaned data. 


### the ending graphs are hard to read. Is there a way to label specific countries kde lines?

Yes

### how do you get the index of something in a row

I do not understand this question, please reach out with more info so that I can help you

### i was trying to do somthing earlier where I got the max of a column but wanted to know somthing else associated with that row

most pandas methods allow you to use `axis` to apply them across rows instead of columns


### cleaning out different data types

Once the data is loaded into pandas, we can use all of the same techniques

### Is there a scenario where a dataset cannot be cleaned using pandas methods?

yes, there are cases where cleaning the already loaded dataframe is not the best way to make the necessary fixes.  One tool that can help is called [Openrefine](https://openrefine.org/) it is an open source program that provides a GUI based interface to clean data, but also outputs a script of what actions were taking, to make the cleaning replicable.

This is something you could learn about and try for the level 3 prepare achievement in your portfolio. One resource for learning it is the [Data Carpentry lesson on Data Cleaning for Ecologists](https://datacarpentry.org/OpenRefine-ecology-lesson/)

### Is there another standard format for sharing cleaned datasets?

.csv is the widely accepted preferred format for well strucutured tabular data that is of reasonable size.  If it is too large, a database might (next week!) can be useful.

### With extremely large datasets, is there a way to find what pieces of the dataset are missing values other than manually checking

Using strategies like we did to day, by testing dropping and the EDA strategies that we used last week can help.  If it is a very large dataset, you may need to use more powerful system, but the basics work out the same. 

Also, remember in most contexts, you will have some relevant domain knowledge that can help you. 
